{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79495c90",
   "metadata": {},
   "source": [
    "# Running p-value computation with python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999e96f",
   "metadata": {},
   "source": [
    "This is an *VariantSpark* example notebook.\n",
    "\n",
    "One of the main applications of VariantSpark is discovery of genomic variants correlated with a response variable (e.g. case vs control) using random forest gini importance.\n",
    "\n",
    "The `chr22_1000.vcf` is a very small sample of the chromosome 22 VCF file from the 1000 Genomes Project.\n",
    "\n",
    "`chr22-labels-hail.csv` is a CSV file with sample response variables (labels). In fact the labels directly represent the number of alternative alleles for each sample at a specific genomic position. E.g.: column x22_16050408 has labels derived from variants in chromosome 22 position 16050408. We would expect then that position 22:16050408 in the VCF file is strongly correlated with the label x22_16050408.\n",
    "\n",
    "Both data sets are located in the `..\\\\data` directory.\n",
    "\n",
    "This notebook demonstrates how to run importance analysis on these data with *VariantSpark* Hail integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd18fb5",
   "metadata": {},
   "source": [
    "Step 1: Create a `HailContext` using `SparkContext` object (here injected as `sc`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc96fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhail\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhl\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvarspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhail\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mvshl\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mvshl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/VariantSpark/python/varspark/hail/context.py:17\u001b[0m, in \u001b[0;36minit\u001b[0;34m(quiet, spark_conf, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\" Initialises hail context with variant-spark support.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    :param kwargs: same as for hail.init()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m jars \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m spark_conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;28;01mif\u001b[39;00m spark_conf \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 17\u001b[0m vs_jar_path \u001b[38;5;241m=\u001b[39m \u001b[43mvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_jar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(vs_jar_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m vs_jar_path\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "File \u001b[0;32m~/dev/VariantSpark/python/varspark/etc.py:19\u001b[0m, in \u001b[0;36mfind_jar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(jars_dir):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# then it can be an develoment install\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     jars_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     16\u001b[0m                                                                os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpardir,\n\u001b[1;32m     17\u001b[0m                                                                             os\u001b[38;5;241m.\u001b[39mpardir,\n\u001b[1;32m     18\u001b[0m                                                                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjars_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*-all.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from unidip import UniDip #pip\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import patches\n",
    "\n",
    "from varspark.pvalues_calculation import * \n",
    "import hail as hl\n",
    "import varspark.hail as vshl\n",
    "vshl.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d4198",
   "metadata": {},
   "source": [
    "Step 2: Load Hail variant dataset `vds` from a sample `.vcf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = hl.import_vcf('../data/chr22_1000.vcf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd551f",
   "metadata": {},
   "source": [
    "Step 3: Load labels into Hail table `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb31a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hl.import_table('../data/chr22-labels-hail.csv', impute = True, delimiter=\",\").key_by('sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfec65c",
   "metadata": {},
   "source": [
    "Step 4: Annotate dataset samples with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = vds.annotate_cols(label = labels[vds.s])\n",
    "vds.cols().show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23297bf7",
   "metadata": {},
   "source": [
    "Step 5: Build the random forest model with `label.x22_16050408` as the respose variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4adf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = vshl.random_forest_model(y=vds.label['x22_16050408'],\n",
    "                x=vds.GT.n_alt_alleles(), seed = 13, mtry_fraction = 0.05, min_node_size = 5, max_depth = 10)\n",
    "rf_model.fit_trees(100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a263be",
   "metadata": {},
   "source": [
    "Step 6: Display the results: print OOB error calculated variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c335f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OOB error: %s\" % rf_model.oob_error())\n",
    "impTable = rf_model.variable_importance()\n",
    "impTable.order_by(hl.desc(impTable.importance)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f8c01",
   "metadata": {},
   "source": [
    "Step 7: Obtaiin the variable importance table and their `splitCount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PValueCalculator:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self._df = df\n",
    "\n",
    "    @classmethod\n",
    "    def from_imp_table(cls,impTable):\n",
    "        impDf = impTable.filter(impTable.splitCount >= 1).to_spark(flatten=False).toPandas()\n",
    "        df = impDf.assign(logImportance = np.log(impDf.importance))\n",
    "        return PValueCalculator(df)\n",
    "\n",
    "    def plot_log_densities(self, ax, min_split_count = 1, max_split_count=6, palette = 'Set1',\n",
    "                      xLabel = 'log(importance)', yLabel = 'density'):\n",
    "        #TODO test preconditions\n",
    "        no_lines = max_split_count - min_split_count + 1\n",
    "        colors= sns.mpl_palette(palette, no_lines)\n",
    "        df = self._df\n",
    "        for i,c in zip(range(min_split_count, max_split_count + 1), colors):\n",
    "            sns.kdeplot(df.logImportance[df.splitCount >= i],\n",
    "                        ax = ax, c=c, bw_adjust=0.5) #bw low show sharper distributions\n",
    "    \n",
    "    \n",
    "        #ax.legend(labels=range(1,n_lines), bbox_to_anchor=(1,1))\n",
    "        ax.set_xlabel(xLabel)\n",
    "        ax.set_ylabel(yLabel)\n",
    "\n",
    "\n",
    "    def plot_log_hist(self, ax, split_count, bins = 100,\n",
    "                          xLabel = 'log(importance)', yLabel = 'count'):\n",
    "        # check preconditions\n",
    "        df = self._df\n",
    "        sns.histplot(df.logImportance[df.splitCount >= split_count], ax = ax, bins=bins)\n",
    "        ax.set_xlabel(xLabel)\n",
    "        ax.set_ylabel(yLabel)\n",
    "        \n",
    "        \n",
    "    def compute_p_values(self, countThreshold = 2, pValue = 0.05, **kwargs):\n",
    "        impDfWithLog = self._df[self._df.splitCount >= countThreshold]\n",
    "        pValueResult = run_it_importances(impDfWithLog.logImportance, pValue)\n",
    "        #return impDfWithLog.assign(pValue =  pValueResult['ppp'])\n",
    "        return (impDfWithLog.assign(pvalue = pValueResult['ppp']), pValueResult)   \n",
    "    \n",
    "    \n",
    "    def find_split_count_th(self, min_split_count = 1, max_split_count=6, ntrials=1000):\n",
    "        df = self._df\n",
    "        for splitCountThreshold in range(min_split_count,max_split_count + 1):\n",
    "            dat = np.msort(df[df['splitCount']>splitCountThreshold]['logImportance'])\n",
    "            intervals = UniDip(dat, ntrials=ntrials).run() #ntrials can be increased to achieve higher robustness\n",
    "            if len(intervals) <= 1: \n",
    "                break\n",
    "        # TODO: check if converged !!!\n",
    "        return splitCountThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pValCalc = PValueCalculator.from_imp_table(impTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a31f9",
   "metadata": {},
   "source": [
    "Setp 8: Determine the cutoff (of how many times a variable was used to split a tree) to get a unimodal density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1759b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoSplitCountTh = pValCalc.find_split_count_th()\n",
    "print(\"The automatically selected SplitCount Threshold is %s\" % autoSplitCountTh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8ce52",
   "metadata": {},
   "source": [
    "Step 9: Display (A) Density graphs of the Gini importance scores with different colours indicating the SplitCounts of a variable. (B) Histogram of Gini importances scores of variables with `SplitCountThreshold` selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2089ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize'] = [15, 10]\n",
    "#SMALL_SIZE = 12\n",
    "#MEDIUM_SIZE = 14\n",
    "#BIGGER_SIZE = 16\n",
    "#plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "#plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "#plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "#plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "#plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "#plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "#plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5), layout='constrained')\n",
    "pValCalc.plot_log_densities(ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b638d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(figsize=(10, 5), layout='constrained')\n",
    "pValCalc.plot_log_hist(ax2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32b30e",
   "metadata": {},
   "source": [
    "Step 10: Preparing the DataFrame for the variant p-value calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1d967",
   "metadata": {},
   "source": [
    "Step 11: Computing p-values and keeping the significant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalueDF, info = pValCalc.compute_p_values(countThreshold = 2, maxFRD = 0.2)\n",
    "print(\"C = %s\" % info['C'])\n",
    "pvalueDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalueDF.sort_values('pvalue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
